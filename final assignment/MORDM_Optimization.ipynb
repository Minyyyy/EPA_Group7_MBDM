{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Scenario Multi-objective Robust Decision Making optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the Multi-Scenario Multi-objective Robust Decision Making (MORDM) optimization process. First, the wort-case reference scenarios are identified. Second, the minimum and maximum values for the outcomes are estimated. Third, the previous are used to perform Multi-Scenario MORDM optimization.\n",
    "\n",
    "This notebook is concerned with the identification of the optimal values for an efficient and representative optimisation. The objective outcomes to be optimized aim to minimize the aggregated Expected Annual Damage, Dike Investment Costs, and Expected Number of Deaths. The other outcomes RfR Investment Costs and Evacuation Costs were not minimized because it is in the interest of the Environmental Interest group to consider policies in which RfR and Evacuation are implemented.\n",
    "\n",
    "Besides looking for a policy that protects the interests of the client, it is also important to search for the most favourable and acceptable policy for other involved actors. In light of this, the MORDM framework can search for promising strategies using a a Many-Objective Evolutionary Algorithm (MOEA) in multiple reference scenarios (Kasprzyk et al.2013; Watson and Kasprzyk2017; Bartholomew and J. H. Kwakkel2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Worst-case reference scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we identify which are the top 5 worst-case scenarios in terms of Expected Annual Damage and Expected Number of Deaths from the base case. Finding effective policies under those scenarios will guarantee to a certain extent that they will also be effective under the rest of scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ema_workbench import load_results\n",
    "from ema_workbench.em_framework.evaluators import perform_experiments\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "from ema_workbench.util import ema_logging\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume, EpsilonProgress)\n",
    "from ema_workbench import (Model, CategoricalParameter,\n",
    "                           ScalarOutcome, IntegerParameter, RealParameter)\n",
    "from ema_workbench import (Model, MultiprocessingEvaluator, SequentialEvaluator,\n",
    "                           Constraint, Policy, Scenario)\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\minyr\\\\Documents\\\\Studie\\\\MADE\\\\Electives\\\\Model-based_decision_making\\\\EPA_Group7_MBDM-main\\\\EPA_Group7_MBDM-main\\\\results\\\\basecase_results.tarbase_case_5_objectives_no_policy.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the base case results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m experiments, outcomes \u001b[38;5;241m=\u001b[39m \u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../results/basecase_results.tarbase_case_5_objectives_no_policy.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m outcomes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(outcomes)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ema_workbench\\util\\utilities.py:50\u001b[0m, in \u001b[0;36mload_results\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mem_framework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutcomes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractOutcome, register\n\u001b[0;32m     48\u001b[0m file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(file_name)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr:gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUTF8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m archive:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         f \u001b[38;5;241m=\u001b[39m archive\u001b[38;5;241m.\u001b[39mextractfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1629\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1628\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown compression type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m comptype)\n\u001b[1;32m-> 1629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(name, filemode, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1632\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1675\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip module is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1674\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1675\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\gzip.py:173\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    171\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\minyr\\\\Documents\\\\Studie\\\\MADE\\\\Electives\\\\Model-based_decision_making\\\\EPA_Group7_MBDM-main\\\\EPA_Group7_MBDM-main\\\\results\\\\basecase_results.tarbase_case_5_objectives_no_policy.csv'"
     ]
    }
   ],
   "source": [
    "# load the base case results\n",
    "experiments, outcomes = load_results('../results/basecase_results.tarbase_case_5_objectives_no_policy.csv') \n",
    "outcomes_df = pd.DataFrame.from_dict(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the experiment results that are within the established threshold in the Scenario Discovery section in the base case analysis.\n",
    "# These are the ones that are simultaneously among the 25% of the worst outcomes for death and damage costs as well as \n",
    "# being among the 50% of the worst outcomes with high disadvantaged locations.\n",
    "y3 = pd.read_csv('../results/boolean_worst_scenarios.csv').iloc[:, 1:]\n",
    "\n",
    "# Convert Series to a list\n",
    "y = np.array(y3).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out experiments in the base case result which obey the established threshold\n",
    "experiments_of_interest = experiments.loc[y]\n",
    "outcomes_df = pd.DataFrame({k:v[y] for k,v in outcomes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate over time and locations\n",
    "def aggregate_df(df):\n",
    "    df_aggregate_time = pd.DataFrame()\n",
    "    df_aggregate_time_location = pd.DataFrame()\n",
    "    locations = ['A.1', 'A.2', 'A.3', 'A.4', 'A.5']\n",
    "    step = ['0', '1', '2']\n",
    "    metrics = ['Expected Annual Damage', 'Dike Investment Costs', 'Expected Number of Deaths', 'RfR Total Costs', 'Expected Evacuation Costs']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric == \"RfR Total Costs\" or metric == \"Expected Evacuation Costs\":\n",
    "\n",
    "            columns = [metric + ' ' + time for time in step]\n",
    "\n",
    "            df_aggregate_time_location[metric + ' time aggregate'] = df[columns].sum(axis=1)\n",
    "        else:\n",
    "            for location in locations:\n",
    "                columns = [location + '_' + metric + ' ' + time for time in step]\n",
    "\n",
    "                df_aggregate_time[location + '_' + metric + ' time aggregate'] = df[columns].sum(axis=1)\n",
    "            \n",
    "            columns_locations = [location + '_' + metric + ' time aggregate' for location in locations]\n",
    "            df_aggregate_time_location[metric + ' time location aggregate'] = df_aggregate_time[columns_locations].sum(axis=1)\n",
    "                    \n",
    "\n",
    "    return df_aggregate_time, df_aggregate_time_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the outcome values\n",
    "df_agg_t, df_agg_tl = aggregate_df(outcomes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we select the worst case scenarios over which we will optimize. In line with the maximin criterion, we rank all the different columns in the outcomes dataframe from highest to lowest value using eually weighted integer ranking. Then, we sum up the rank integer across all KPIs that belong to the same acenario and chose the scenarios which have the highest rank, i.e. the ones that perform the worst.  Specifically, 10 KPIs were ranked, being the time-aggregated expected number of deaths and expected damage costs. The other KPIs were not considered as the scenario discovery was performed on the base case and hence all policy costs will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that we're ranking on the base case, the dike investment costs are zero. Therefore, we drop those columns.\n",
    "dike_inv = ['A.1_Dike Investment Costs time aggregate', 'A.2_Dike Investment Costs time aggregate', \n",
    "            'A.3_Dike Investment Costs time aggregate', 'A.4_Dike Investment Costs time aggregate', 'A.5_Dike Investment Costs time aggregate']\n",
    "\n",
    "df_agg_t.drop(columns=dike_inv, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rank each column and sum over each row value to obtain the rank of each scenario for Expected annual damage and Expected number of deaths\n",
    "rank_df = pd.DataFrame()\n",
    "for column in df_agg_t.columns:\n",
    "    rank_df[column] = df_agg_t[column].rank()\n",
    "\n",
    "rank_df['Final rank'] = rank_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to sort out the values for the top 5 worst scenarios, we retrieve their index to then filter them out from the experiments \n",
    "worst_5_df = rank_df.sort_values(by='Final rank',ascending=False).head()\n",
    "worst_5_ind = list(worst_5_df.index)\n",
    "\n",
    "# Finally, we filter the worst scenarios out and only get back the first 19 columns, which correspond to the uncertain external factors in each scenario\n",
    "scenarios_df = experiments_of_interest.iloc[worst_5_ind,:19]\n",
    "indexes_scenarios = scenarios_df.index.to_list()\n",
    "indexes_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the scenarios in the workbench\n",
    "scenarios = [Scenario(f\"{index}\", **row) for index, row in scenarios_df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Estimation of minimum and maximum values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the values of the uncertain external factors that correspond to the top 5 worst case scenarios, the only missing thing before we can start optimizing is the estimation of the minimum and maximum values of the KPIs we will optimize. These values are needed for the two following reason:\n",
    "1. Determine the $\\epsilon$ values for the $\\epsilon-NSGAII$ grid\n",
    "2. Determine the Hypevolume space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the maximum and minimum values found across performing experiments for 100 policies across 400 scenarios in the policy lever exploration\n",
    "import pickle\n",
    "with open('../results/min_max_range_basecase.pickle', 'rb') as f:\n",
    "    max_range_bc, min_range_bc = pickle.load(f)\n",
    "with open('../results/min_max_range_randompolicies.pickle', 'rb') as f:\n",
    "    max_range_rp, min_range_rp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_range = pd.concat([max_range_bc, max_range_rp], axis=1)\n",
    "min_range = pd.concat([min_range_bc, min_range_rp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_range = max_range.max(axis=1)\n",
    "min_range = min_range.min(axis=1)\n",
    "\n",
    "# Given that we are not optimizing over RfR Total Costs we are not interested in its maximum and minimum values\n",
    "max_range.drop(labels=['RfR Total Costs time aggregate'], inplace=True)\n",
    "min_range.drop(labels=['RfR Total Costs time aggregate'], inplace=True)\n",
    "\n",
    "# Similarly, given that we are not optimizing over Expected Evacuation Costs we are not interested in its maximum and minimum values\n",
    "max_range.drop(labels=['Expected Evacuation Costs time aggregate'], inplace=True)\n",
    "min_range.drop(labels=['Expected Evacuation Costs time aggregate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see what are the values\n",
    "max_range, min_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Execute Multi-Scenario MORDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is now ready to optimize over the selected KPIs. Given that we don't want to optimize over all the outcomes defined in the original problem formulation, we created an alternative problem formulation (problem_formulation_altered). There are two main differences from the original:\n",
    "1. It only contains Expected Annual Damage,  Dike Investment Costs, and Expected Number of Deaths as outcomes\n",
    "2. The user can directly input the maximum and minium values for the expected_range in the order specified in 1.\n",
    "\n",
    "Besides optimizing for a policy that protects the interests of the client, it is also important to search for the most favourable and acceptable policy for other involved actors, especially Rijkswaterstaat and Delta Commission given they have veto power. Therefore, a constraint has been set on the Expected Number of Deaths by which the experiment outcomes cannot be above 0.01\\% * 3 time stages * 5 locations = 0.15\\% = max_num_deaths. This constraints look to meet the requirement set by the Delta Commission by which there cannot be more than 0.01\\% deaths per location and time stage. As a consequence of requiring to aggregate the outcomes, it is possible that this constraint is not met in the specific locations and outcomes, but at least ensures that overall this condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from problem_formulation_altered import get_model_for_problem_formulation_altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the altered formulation with the maximum and minimum ranges\n",
    "dike_model, planning_steps = get_model_for_problem_formulation_altered(min_range,max_range)\n",
    "\n",
    "deaths = ['Expected Number of Deaths']\n",
    "\n",
    "max_num_deaths = 0.0001 * 3 * 5\n",
    "\n",
    "# Adding the constraint for the allowed maximum number of deaths defined by the Delta Commission. By setting this as a constraint, the optimization will not\n",
    "# return policies that do not meet this criterion.\n",
    "constraints = [Constraint(\"Max number of deaths\", outcome_names=deaths,\n",
    "                          function=lambda x:max(0, x-max_num_deaths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the code to run the optimization. Results were saved and therefore it is commented out.\n",
    "\n",
    "\n",
    "# ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "# The Hypervolume space has already been defined in the get_problem_formulation_altered function\n",
    "# convergence_metrics = [HyperVolume.from_outcomes(dike_model.outcomes),\n",
    "#                        EpsilonProgress()]\n",
    "\n",
    "# nfe = 7500\n",
    "# results_deep = []\n",
    "# convergence_all = []\n",
    "\n",
    "# for scenario in scenarios:\n",
    "#     with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "#         results_runs, convergence = evaluator.optimize(nfe=nfe, searchover='levers',\n",
    "#                                         epsilons=[1e7, 1e6, 0.00001],\n",
    "#                                         convergence=convergence_metrics, reference=scenario, constraints=constraints)\n",
    "                                        \n",
    "        \n",
    "#         results_deep.append(results_runs)\n",
    "#         convergence_all.append(convergence)\n",
    "\n",
    "# from ema_workbench import save_results\n",
    "\n",
    "# for i in range(len(results_deep)):\n",
    "#     save_results((results_deep[i], convergence_all[i]), f'../results/mordm_7500_rp_scenario{i}.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the $\\epsilon$ have been iteratively determined by performing several short runs and studying the amount of solutions obtained by decreasing the maximum values for each outcome in 2 orders of magnitude. Closer attention was payed to the $\\epsilon$ value for Expected Number of Deaths as we were interested in finding multiple optimal policies that obeyed the threshold. For that reason, the chosen $\\epsilon$ is one order of magnitude lower than the constraint value.\n",
    "\n",
    "Multi-Scenario MORDM succesfully found 42 policies that met the requirements. These policies are not evenly optimal, and trade-offs will need to be taken. To analyse the results in more depth, the notebook \"Reevaluation\" analyses the found policies under a larger space of deep uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now load the saved results\n",
    "from ema_workbench import load_results\n",
    "\n",
    "results_deep = []\n",
    "convergence_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    result, convergence = load_results(f'../results/mordm_7500_rp_scenario{i}.tar.gz')\n",
    "    results_deep.append(result)\n",
    "    convergence_all.append(convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the convergence metrics to check if the algorithm has converged\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(12,6))\n",
    "\n",
    "for i, con in enumerate(convergence_all):\n",
    "    ax1.plot(con['nfe'], con['epsilon_progress'], label=f'Scenario index: {indexes_scenarios[i]}')\n",
    "    ax2.plot(con['nfe'], con['hypervolume'], label=f'Scenario index: {indexes_scenarios[i]}')\n",
    "\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(1.08, 0.9))\n",
    "plt.savefig('../images/MSMORDM_convergence.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyse the epsilon-progress and hypervolume convergence metrics to track whether the Multi-Scenario MORDM has converged - i.e. no better solutions can be found within the demarcated policy space - to the possible optimum solutions.\n",
    "\n",
    "\n",
    "We can see that the epsilon-progress has not fully converged while the hypervolume metric has converged. This suggests that the ranges established for the hypervolume metric were adequate. The epsilon-progress hs started to stabilise for some of the scenarios, however, to ensure that no better solutions can be found, the algorithm should be executed with a larger number of funtion evaluation. Given that this is an exercise of trial and error and due to time constraints, we assume the algorithm found the optimized solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
